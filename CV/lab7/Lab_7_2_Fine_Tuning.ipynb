{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6uOfRzgAOal"
      },
      "source": [
        "# Lab 7.2 Fine-Tuning\n",
        "\n",
        "In this notebook you will explore fine-tuning a CNN to classify pet breeds."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyzt-DP1N2Eb"
      },
      "source": [
        "Here is some code to download a prepared version of the [Oxford-IIIT Pet dataset](https://www.robots.ox.ac.uk/~vgg/data/pets/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Y7DK-ZnOphy1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5efff97a-c7f5-4634-f40e-ab9f312540e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-11-19 02:53:37--  https://www.dropbox.com/scl/fi/p49ifha27c2u3uptfj42w/oxford_pets_corrected.zip?rlkey=dwk3dsptzir8v846imsq6bgw3&dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.81.18, 2620:100:6031:18::a27d:5112\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.81.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ucc3febd5c619acab7f1578dda71.dl.dropboxusercontent.com/cd/0/inline/CeqZ45QyK9RB2kVdit_L-03u3_fTjSieOIrWFYEdrCAmqBWusdkajpXLWBNEWm9NKthtcVIEcvVVWKphOy-KlHz4QmgOWBc29qapC-qE8O1tmkdVr-aBqgtAfWP-keAornCCcNhKHcy-KBhuSckH2nSD/file?dl=1# [following]\n",
            "--2024-11-19 02:53:38--  https://ucc3febd5c619acab7f1578dda71.dl.dropboxusercontent.com/cd/0/inline/CeqZ45QyK9RB2kVdit_L-03u3_fTjSieOIrWFYEdrCAmqBWusdkajpXLWBNEWm9NKthtcVIEcvVVWKphOy-KlHz4QmgOWBc29qapC-qE8O1tmkdVr-aBqgtAfWP-keAornCCcNhKHcy-KBhuSckH2nSD/file?dl=1\n",
            "Resolving ucc3febd5c619acab7f1578dda71.dl.dropboxusercontent.com (ucc3febd5c619acab7f1578dda71.dl.dropboxusercontent.com)... 162.125.81.15, 2620:100:6031:15::a27d:510f\n",
            "Connecting to ucc3febd5c619acab7f1578dda71.dl.dropboxusercontent.com (ucc3febd5c619acab7f1578dda71.dl.dropboxusercontent.com)|162.125.81.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/CepZsISOgV3cpCf0EYD4yinEtyiaks-p5k7TJMi-OOStwbMZu92g9xXsVITRBuRZTX0flCzPIB1if3nFezTig6wEBe_bM2ew9_1S1k-2vCDcSj7B1QpR3w0m9WzV2gyNXkhbyND2e6y-DYYGwOvkifag2PiZfo1QKC5j0DEhMuL-WbLni-_49sEdQkxU2TAdmtTPw5OYaQmzkiYRdCCUEjz4_a9I_2RQfgBo2MZCp9HAVS2mPzay5etAtZ4XT60ZXjpAiYVOIEXGtQkqtjoTRi3kvQBZZRQVfx9rTtOGtXsPwZvadSpfNYhpoIa2liSTcSON8f5SIku2kGAJgMqMX2TU6J1zo6j2-Lwj4vGPO1JSAETjA6gkmGSorZWc1ACbiGY/file?dl=1 [following]\n",
            "--2024-11-19 02:53:39--  https://ucc3febd5c619acab7f1578dda71.dl.dropboxusercontent.com/cd/0/inline2/CepZsISOgV3cpCf0EYD4yinEtyiaks-p5k7TJMi-OOStwbMZu92g9xXsVITRBuRZTX0flCzPIB1if3nFezTig6wEBe_bM2ew9_1S1k-2vCDcSj7B1QpR3w0m9WzV2gyNXkhbyND2e6y-DYYGwOvkifag2PiZfo1QKC5j0DEhMuL-WbLni-_49sEdQkxU2TAdmtTPw5OYaQmzkiYRdCCUEjz4_a9I_2RQfgBo2MZCp9HAVS2mPzay5etAtZ4XT60ZXjpAiYVOIEXGtQkqtjoTRi3kvQBZZRQVfx9rTtOGtXsPwZvadSpfNYhpoIa2liSTcSON8f5SIku2kGAJgMqMX2TU6J1zo6j2-Lwj4vGPO1JSAETjA6gkmGSorZWc1ACbiGY/file?dl=1\n",
            "Reusing existing connection to ucc3febd5c619acab7f1578dda71.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 790773793 (754M) [application/binary]\n",
            "Saving to: ‘oxford_pets.zip’\n",
            "\n",
            "oxford_pets.zip     100%[===================>] 754.14M  21.1MB/s    in 40s     \n",
            "\n",
            "2024-11-19 02:54:20 (18.6 MB/s) - ‘oxford_pets.zip’ saved [790773793/790773793]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "if not os.path.exists('oxford_pets.zip'):\n",
        "  !wget \"https://www.dropbox.com/scl/fi/p49ifha27c2u3uptfj42w/oxford_pets_corrected.zip?rlkey=dwk3dsptzir8v846imsq6bgw3&dl=1\" -O oxford_pets.zip\n",
        "  !unzip -qq oxford_pets.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "rysmvQcVme6i"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from keras import Sequential\n",
        "from keras.optimizers import SGD, Adam\n",
        "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, Flatten, Lambda\n",
        "from keras.regularizers import L2\n",
        "from tensorflow.keras.layers import RandomFlip, RandomZoom, RandomRotation\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input, VGG16"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls oxford_pets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZt8w0lIWPKA",
        "outputId": "8d82377b-ffe7-4500-ecee-cb4cd51c3c00"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test  train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58FKxOHhN2Ec"
      },
      "source": [
        "### Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmfeuXNzN2Ed"
      },
      "source": [
        "1. Set up the data loaders.\n",
        "\n",
        "- Use `keras.utils.image_dataset_from_directory` to create training, validation and test datasets.  \n",
        "- Set the image size to (224,224) and use a validation split of 0.1.\n",
        "- Remember to use the same random seed for training and validation splits."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = keras.preprocessing.image_dataset_from_directory('oxford_pets/train',\n",
        "                                                            image_size=(224,224),\n",
        "                                                            validation_split=0.1,\n",
        "                                                            subset='training',\n",
        "                                                            seed=27)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H2Fr936vWGpp",
        "outputId": "0754e33f-df8f-4419-9020-88fc9a2e4bf7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6639 files belonging to 35 classes.\n",
            "Using 5976 files for training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_ds = keras.preprocessing.image_dataset_from_directory('oxford_pets/train',\n",
        "                                                            image_size=(224,224),\n",
        "                                                            validation_split=0.1,\n",
        "                                                            subset='validation',\n",
        "                                                            seed=27)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTJINihvW9sg",
        "outputId": "ca30d869-0189-44e0-ef50-c475af9cd343"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6639 files belonging to 35 classes.\n",
            "Using 663 files for validation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_ds = keras.preprocessing.image_dataset_from_directory('oxford_pets/test',\n",
        "                                                            image_size=(224,224),\n",
        "                                                            seed=27)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPdcbiIEY4AG",
        "outputId": "a2887df6-7062-45a1-e1ba-04c61f42c6d5"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 739 files belonging to 35 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICpgpWTxAj3_"
      },
      "source": [
        "2. Create a `Sequential` model with the pre-trained `VGG16` network.\n",
        "\n",
        "The model should have the following layers:\n",
        "- Input layer\n",
        "- One or more [data augmentation](https://keras.io/api/layers/preprocessing_layers/image_augmentation/) layers such as `RandomFlip`, `RandomZoom`, `RandomRotation`, etc.\n",
        "- VGG16 preprocess function inside a `Lambda` layer\n",
        "- VGG16 layer: don't include top; use `max` pooling.\n",
        "- Dense layer with 35 outputs and correct activation function for multi-class classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "5TKMi61_ml8v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d3d26f2-ccc7-448b-85cb-c366e610f41d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m58889256/58889256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "model = Sequential([\n",
        "    Input(shape=(224,224,3)), #0\n",
        "    RandomZoom(.2),#1\n",
        "      # Preprocessing layer for VGG16\n",
        "    Lambda(preprocess_input),#2\n",
        "\n",
        "    # VGG16 base model\n",
        "    VGG16(include_top=False, weights='imagenet', pooling='max'), #3\n",
        "\n",
        "    # Dense output layer\n",
        "    Dense(35, activation='softmax') #4\n",
        "])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Btj59JlN2Ed"
      },
      "source": [
        "Set the VGG16 part of the network to be fixed by setting the `trainable` attribute of VGG16 layer to `False`.  (You can access the layers of the model with `model.layers`.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "UXVqqX8yN2Ed"
      },
      "outputs": [],
      "source": [
        "model.layers[3].trainable = False  # VGG16 is the 4th layer in the Sequential model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pBdlU3RN2Ed"
      },
      "source": [
        "Check the model summary to make sure that the VGG16 layer is trainable (it should report a very large number of non-trainable parameters, and a small number of trainable parameters.)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "id": "DAhaJFg1mvih",
        "outputId": "9a58c72c-67f7-45f1-999c-e3185439615e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ random_zoom (\u001b[38;5;33mRandomZoom\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lambda (\u001b[38;5;33mLambda\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ vgg16 (\u001b[38;5;33mFunctional\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │      \u001b[38;5;34m14,714,688\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m35\u001b[0m)                  │          \u001b[38;5;34m17,955\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ random_zoom (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">RandomZoom</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lambda (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ vgg16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │      <span style=\"color: #00af00; text-decoration-color: #00af00\">14,714,688</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">35</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">17,955</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m14,732,643\u001b[0m (56.20 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,732,643</span> (56.20 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m14,714,688\u001b[0m (56.13 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,714,688</span> (56.13 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m17,955\u001b[0m (70.14 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">17,955</span> (70.14 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jI_7Dm4oN2Ee"
      },
      "source": [
        "Compile the model with multi-class classification loss and accuracy metric.  You can use Adam with learning rate 3e-4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "MWWeiZAA_MG-"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=Adam(3e-4),\n",
        "               loss='sparse_categorical_crossentropy',\n",
        "               metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZdx7Yr7N2Ee"
      },
      "source": [
        "4. Evaluate the model on the test set and check that the accuracy is about $1/35=.029$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTnAvRKk_MG_",
        "outputId": "be3454a1-0d86-44bb-d291-a224dad97f22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 189ms/step - accuracy: 0.0265 - loss: 161.6640\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[162.3706512451172, 0.025710418820381165]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "model.evaluate(test_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZNb41ImN2Ee"
      },
      "source": [
        "5. Now train the model on the training set (don't forget to include the validation set) for 20 epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZJq3eqiAN_d",
        "outputId": "e094f646-d2db-44b5-f2c0-d76b87769824"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 512ms/step - accuracy: 0.0280 - loss: 8.6304 - val_accuracy: 0.0226 - val_loss: 3.5553\n",
            "Epoch 2/20\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 452ms/step - accuracy: 0.0256 - loss: 3.5554 - val_accuracy: 0.0226 - val_loss: 3.5553\n",
            "Epoch 3/20\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 453ms/step - accuracy: 0.0264 - loss: 3.5554 - val_accuracy: 0.0226 - val_loss: 3.5553\n",
            "Epoch 4/20\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 453ms/step - accuracy: 0.0263 - loss: 3.5554 - val_accuracy: 0.0226 - val_loss: 3.5553\n",
            "Epoch 5/20\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 453ms/step - accuracy: 0.0246 - loss: 3.5554 - val_accuracy: 0.0226 - val_loss: 3.5553\n",
            "Epoch 6/20\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 452ms/step - accuracy: 0.0266 - loss: 3.5554 - val_accuracy: 0.0226 - val_loss: 3.5553\n",
            "Epoch 7/20\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 462ms/step - accuracy: 0.0248 - loss: 3.5554 - val_accuracy: 0.0226 - val_loss: 3.5553\n",
            "Epoch 8/20\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 453ms/step - accuracy: 0.0252 - loss: 3.5554 - val_accuracy: 0.0226 - val_loss: 3.5553\n",
            "Epoch 9/20\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 451ms/step - accuracy: 0.0249 - loss: 3.5554 - val_accuracy: 0.0226 - val_loss: 3.5553\n",
            "Epoch 10/20\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 452ms/step - accuracy: 0.0270 - loss: 3.5554 - val_accuracy: 0.0226 - val_loss: 3.5553\n",
            "Epoch 11/20\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 452ms/step - accuracy: 0.0261 - loss: 3.5554 - val_accuracy: 0.0226 - val_loss: 3.5553\n",
            "Epoch 12/20\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 453ms/step - accuracy: 0.0256 - loss: 3.5554 - val_accuracy: 0.0226 - val_loss: 3.5553\n",
            "Epoch 13/20\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 453ms/step - accuracy: 0.0247 - loss: 3.5554 - val_accuracy: 0.0226 - val_loss: 3.5553\n",
            "Epoch 14/20\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 453ms/step - accuracy: 0.0265 - loss: 3.5554 - val_accuracy: 0.0226 - val_loss: 3.5553\n",
            "Epoch 15/20\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 452ms/step - accuracy: 0.0256 - loss: 3.5554 - val_accuracy: 0.0226 - val_loss: 3.5553\n",
            "Epoch 16/20\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 452ms/step - accuracy: 0.0255 - loss: 3.5554 - val_accuracy: 0.0226 - val_loss: 3.5553\n",
            "Epoch 17/20\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 452ms/step - accuracy: 0.0260 - loss: 3.5554 - val_accuracy: 0.0226 - val_loss: 3.5553\n",
            "Epoch 18/20\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 462ms/step - accuracy: 0.0255 - loss: 3.5554 - val_accuracy: 0.0226 - val_loss: 3.5553\n",
            "Epoch 19/20\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 452ms/step - accuracy: 0.0253 - loss: 3.5554 - val_accuracy: 0.0226 - val_loss: 3.5553\n",
            "Epoch 20/20\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 452ms/step - accuracy: 0.0264 - loss: 3.5554 - val_accuracy: 0.0226 - val_loss: 3.5553\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7fe03009f490>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "model.fit(train_ds, validation_data=val_ds, epochs=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6bQXPSKN2Ee"
      },
      "source": [
        "6. Evaluate the accuracy of the fine-tuned model on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZVPh1cjk0i2u",
        "outputId": "dcde53f4-5b4e-4e3b-cc6a-7bdcee536d63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 148ms/step - accuracy: 0.0237 - loss: 3.5553\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3.5553481578826904, 0.027063598856329918]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "model.evaluate(test_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zHFu1h7N2Ee"
      },
      "source": [
        "7. The following code will show some test images with the correct and predicted labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gg_WzjG82DBK"
      },
      "outputs": [],
      "source": [
        "images, labels = next(iter(test_ds))\n",
        "preds = model.predict(images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ocfrdyDSrZQD"
      },
      "outputs": [],
      "source": [
        "for im,label,pred in zip(images,labels,preds):\n",
        "  plt.imshow(im.numpy().astype('uint8'))\n",
        "  plt.title(f'correct: {test_ds.class_names[label]} | predicted: {test_ds.class_names[np.argmax(pred)]}')\n",
        "  plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}